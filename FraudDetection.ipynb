{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE-CIS Fraud Detection Study\n",
    "\n",
    "## Introduction\n",
    "Credit or debit cards play an important role in modern life. It makes daily life much easier. However, it is vulnerable to frauds. \n",
    "A robust fraud prevention system can save consumers millions of dollars per year. With a large dataset and better algorithm this system can be improved.\n",
    "\n",
    "The task here is to benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.\n",
    "\n",
    "If successful, weâ€™ll improve the efficiency of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Data is downloaded in the page: [here](https://www.kaggle.com/c/ieee-fraud-detection/data). In the notebook below, we assume the data files are under the *data/* folder under the current folder.\n",
    "\n",
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# personal modules\n",
    "from read import read_data, count_missing\n",
    "from model import train_model_dnn, load_model_dnn\n",
    "\n",
    "# standard modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training and testing data\n",
    "In the data, we realize that a few data fields have *string* inputs, i.e. *email domain*. This is not usable in machine learning tools. These data fields have to be converted to numbers first.\n",
    "\n",
    "For the first time reading the training data, we converted data fields to int numbers and kept them in a separated csv output. Later on, the converted csv file is used to save time.\n",
    "\n",
    "Meanwhile, we record how much time it takes to read the data. It's longer for the first time and much shorter for the later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-28 20:03:47\n",
      "Reading: data/train.csv\n",
      "Reading: data/test.csv\n",
      "Size of training data: 590540, testing data: 506691\n",
      "Time passed:  0 Minutes 52 Seconds.\n"
     ]
    }
   ],
   "source": [
    "cdt0 = datetime.datetime.now()\n",
    "print (cdt0.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "train = read_data(\"train\", converted=True)\n",
    "test = read_data(\"test\", converted=True)\n",
    "print(\"Size of training data: %d, testing data: %d\" %(len(train), len(test)) )                                        \n",
    "\n",
    "time_dif_1 = datetime.datetime.now() - cdt0\n",
    "\n",
    "print (\"Time passed: %2d Minutes %2d Seconds.\"\\\n",
    "       % ( time_dif_1.total_seconds()//60, time_dif_1.total_seconds()%60) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Normalization\n",
    "There are steps to clean the data:\n",
    "* Remove NaN\n",
    "* Replace outlier numbers with upper/lower bound\n",
    "* Normalize variables values to be around 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data normalization\n",
    "And the parameters are calculated and kept. The testing data should apply the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 out of 433 column:  V11\n",
      "100 out of 433 column:  V61\n",
      "150 out of 433 column:  V111\n",
      "200 out of 433 column:  V161\n",
      "250 out of 433 column:  V211\n",
      "300 out of 433 column:  V261\n",
      "350 out of 433 column:  V311\n",
      "400 out of 433 column:  id_26\n",
      "                isint       min0          std0         mean1          std1\n",
      "TransactionDT    True  86400.000  4.617224e+06  7.372311e+06  4.617224e+06\n",
      "TransactionAmt  False      0.251  2.391625e+02  1.350272e+02  2.391625e+02\n",
      "card1            True   1000.000  4.901170e+03  9.898735e+03  4.901170e+03\n",
      "card2           False    100.000  1.577932e+02  3.583452e+02  1.602380e+02\n",
      "card3           False    100.000  1.133644e+01  1.530509e+02  1.166086e+01\n",
      "Time passed:  0 Minutes 20 Seconds.\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values with:\n",
    "#  minimum - 1 if it is int\n",
    "#  minimum - 0.1 * std if it is float\n",
    "cdt0 = datetime.datetime.now()\n",
    "\n",
    "# keep the parameters used in calculation\n",
    "paradict = {}\n",
    "for icol, colname in enumerate(train.columns):\n",
    "    if colname == 'isFraud':\n",
    "        continue\n",
    "    if icol %50 == 0:\n",
    "        print(icol, \"out of\", len(train.columns), \"column: \", colname)\n",
    "    # initialize 5 parameters\n",
    "    paradict[colname] = [0] * 5\n",
    "    unq = train[colname].unique()\n",
    "    isint_col = False\n",
    "    if train[colname].dtype == np.int64 or ( (train[colname].dtype == np.float64) and (np.ceil( unq ) == unq).all() ):\n",
    "        isint_col = True\n",
    "    if train[colname].dtype == np.int64:\n",
    "        train[colname] = train[colname].astype('float64')\n",
    "    \n",
    "    paradict[colname][0] = isint_col\n",
    "    \n",
    "    vmin = train[colname].min()\n",
    "    vstd = train[colname].std()\n",
    "    paradict[colname][1] = vmin\n",
    "    paradict[colname][2] = vstd\n",
    "    \n",
    "    train[colname].fillna(vmin - 1. if isint_col else vmin - 0.1 * vstd, inplace=True)\n",
    "    \n",
    "    vmean = train[colname].mean()\n",
    "    vstd = train[colname].std()\n",
    "    paradict[colname][3] = vmean\n",
    "    paradict[colname][4] = vstd\n",
    "    \n",
    "    train[colname] = train[colname].mask( train[colname] > vmean + 10 * vstd, vmean + 10.5 * vstd)\n",
    "    train[colname] = train[colname].mask( train[colname] < vmean - 10 * vstd, vmean - 10.5 * vstd)\n",
    "    if vstd < 1.e-9:\n",
    "        print(\"std of\", colname, \" is 0???!!!\")\n",
    "    else:\n",
    "        train[colname] = (train[colname] - vmean) / vstd\n",
    "\n",
    "parapd = pd.DataFrame.from_dict(paradict, orient='index', columns=['isint', 'min0', 'std0', 'mean1', 'std1'])\n",
    "print(parapd.head(5))\n",
    "time_dif_1 = datetime.datetime.now() - cdt0\n",
    "print (\"Time passed: %2d Minutes %2d Seconds.\"\\\n",
    "       % ( time_dif_1.total_seconds()//60, time_dif_1.total_seconds()%60) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             isFraud  TransactionDT  TransactionAmt         card1  \\\n",
      "count  590540.000000   5.905400e+05   590540.000000  5.905400e+05   \n",
      "mean        0.034990  -4.620321e-17       -0.003281  1.400535e-16   \n",
      "std         0.183755   1.000000e+00        0.930382  1.000000e+00   \n",
      "min         0.000000  -1.577985e+00       -0.563534 -1.815635e+00   \n",
      "25%         0.000000  -9.410966e-01       -0.383447 -7.915935e-01   \n",
      "50%         0.000000  -1.424748e-02       -0.277042 -4.503713e-02   \n",
      "75%         0.000000   8.390992e-01       -0.041926  8.743352e-01   \n",
      "max         1.000000   1.827683e+00       10.500000  1.733722e+00   \n",
      "\n",
      "              card2         card3         card5         addr1         addr2  \\\n",
      "count  5.905400e+05  5.905400e+05  5.905400e+05  5.905400e+05  5.905400e+05   \n",
      "mean   4.922266e-16  3.908960e-15 -1.854866e-16  1.535583e-15  2.168085e-15   \n",
      "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "min   -1.710733e+00 -4.646705e+00 -2.443264e+00 -1.554649e+00 -2.810880e+00   \n",
      "25%   -9.632243e-01 -2.616400e-01 -7.742930e-01 -7.346862e-01  3.600867e-01   \n",
      "50%    1.032736e-02 -2.616400e-01  6.537145e-01  3.151959e-02  3.600867e-01   \n",
      "75%    9.589162e-01 -2.616400e-01  6.537145e-01  5.103982e-01  3.600867e-01   \n",
      "max    1.508099e+00  6.684672e+00  9.155159e-01  2.364965e+00  9.756566e-01   \n",
      "\n",
      "               dist1  ...       id_30_i       id_31_i        id_33_i  \\\n",
      "count  590540.000000  ...  5.905400e+05  5.905400e+05  590540.000000   \n",
      "mean       -0.002785  ...  5.520922e-17  4.370053e-17      -0.016107   \n",
      "std         0.957989  ...  1.000000e+00  1.000000e+00       0.717681   \n",
      "min        -0.253035  ... -2.470521e-01 -3.840638e-01      -0.181198   \n",
      "25%        -0.253035  ... -2.470521e-01 -3.840638e-01      -0.181198   \n",
      "50%        -0.253035  ... -2.470521e-01 -3.840638e-01      -0.181198   \n",
      "75%        -0.083100  ... -2.470521e-01 -3.840638e-01      -0.181198   \n",
      "max        10.500000  ...  6.776866e+00  4.667006e+00      10.500000   \n",
      "\n",
      "            id_34_i       id_35_i       id_36_i       id_37_i       id_38_i  \\\n",
      "count  5.905400e+05  5.905400e+05  5.905400e+05  5.905400e+05  5.905400e+05   \n",
      "mean   9.866310e-19  3.705882e-17  7.483957e-17  6.357754e-17 -1.129331e-16   \n",
      "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "min   -3.642297e-01 -5.210964e-01 -5.450346e-01 -5.220772e-01 -5.242660e-01   \n",
      "25%   -3.642297e-01 -5.210964e-01 -5.450346e-01 -5.220772e-01 -5.242660e-01   \n",
      "50%   -3.642297e-01 -5.210964e-01 -5.450346e-01 -5.220772e-01 -5.242660e-01   \n",
      "75%   -3.642297e-01 -5.210964e-01 -5.450346e-01 -5.220772e-01 -5.242660e-01   \n",
      "max    8.596078e+00  2.493542e+00  3.807312e+00  3.072968e+00  2.356981e+00   \n",
      "\n",
      "       DeviceType_i   DeviceInfo_i  \n",
      "count  5.905400e+05  590540.000000  \n",
      "mean  -1.047273e-16      -0.001962  \n",
      "std    1.000000e+00       0.977611  \n",
      "min   -5.282883e-01      -0.162091  \n",
      "25%   -5.282883e-01      -0.162091  \n",
      "50%   -5.282883e-01      -0.162091  \n",
      "75%   -5.282883e-01      -0.162091  \n",
      "max    2.232861e+00      10.500000  \n",
      "\n",
      "[8 rows x 433 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of testing data\n",
    "Application of normalization using the parameters calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed:  0 Minutes  9 Seconds.\n"
     ]
    }
   ],
   "source": [
    "# data normalize to 0 - 1.\n",
    "cdt0 = datetime.datetime.now()\n",
    "\n",
    "for colname in test.columns:\n",
    "    if colname == 'isFraud':\n",
    "        continue\n",
    "    visint = parapd.loc[colname, 'isint']\n",
    "    vmin0 = parapd.loc[colname, 'min0']\n",
    "    vstd0 = parapd.loc[colname, 'std0']\n",
    "    test[colname].fillna(vmin0 - 1. if visint else vmin0 - 0.1 * vstd0, inplace=True)\n",
    "    \n",
    "    vmean1 = parapd.loc[colname, 'mean1']\n",
    "    vstd1 = parapd.loc[colname, 'std1']\n",
    "    \n",
    "    test[colname] = test[colname].mask( test[colname] > vmean1 + 10 * vstd1, vmean1 + 10.5 * vstd1)\n",
    "    test[colname] = test[colname].mask( test[colname] < vmean1 - 10 * vstd1, vmean1 - 10.5 * vstd1)\n",
    "    if vstd1 < 1.e-9:\n",
    "        print(\"std of\", colname, \"not found in parameter list!\")\n",
    "    else:\n",
    "        test[colname] = (test[colname] - vmean1) / vstd1\n",
    "\n",
    "time_dif_1 = datetime.datetime.now() - cdt0\n",
    "print (\"Time passed: %2d Minutes %2d Seconds.\"\\\n",
    "       % ( time_dif_1.total_seconds()//60, time_dif_1.total_seconds()%60) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TransactionDT  TransactionAmt          card1          card2  \\\n",
      "count  506691.000000   506691.000000  506691.000000  506691.000000   \n",
      "mean        4.235798       -0.006845       0.011933       0.003846   \n",
      "std         1.030166        0.942928       0.996693       1.007515   \n",
      "min         2.389079       -0.564508      -1.815431      -1.710733   \n",
      "25%         3.335171       -0.397333      -0.791594      -0.969465   \n",
      "50%         4.295297       -0.280467      -0.019533       0.016568   \n",
      "75%         5.192785       -0.041926       0.893106       0.958916   \n",
      "max         5.813458       10.500000       1.733926       1.508099   \n",
      "\n",
      "               card3          card5          addr1          addr2  \\\n",
      "count  506691.000000  506691.000000  506691.000000  506691.000000   \n",
      "mean        0.014451       0.016516      -0.023440      -0.060388   \n",
      "std         1.123117       0.989144       1.017935       1.066948   \n",
      "min        -4.646705      -2.443264      -1.554649      -2.810880   \n",
      "25%        -0.261640      -0.774293      -0.734686       0.360087   \n",
      "50%        -0.261640       0.653715       0.005399       0.360087   \n",
      "75%        -0.261640       0.653715       0.510398       0.360087   \n",
      "max         6.770429       0.915516       2.364965       0.975657   \n",
      "\n",
      "               dist1          dist2  ...        id_30_i        id_31_i  \\\n",
      "count  506691.000000  506691.000000  ...  506691.000000  506691.000000   \n",
      "mean       -0.043086       0.002070  ...       0.101775       0.307618   \n",
      "std         0.811462       0.782049  ...       1.339589       1.457866   \n",
      "min        -0.253035      -0.120502  ...      -0.247052      -0.384064   \n",
      "25%        -0.253035      -0.120502  ...      -0.247052      -0.384064   \n",
      "50%        -0.253035      -0.120502  ...      -0.247052      -0.384064   \n",
      "75%        -0.079071      -0.120502  ...      -0.247052      -0.267501   \n",
      "max        10.500000      10.500000  ...       7.807040       4.861278   \n",
      "\n",
      "             id_33_i        id_34_i        id_35_i        id_36_i  \\\n",
      "count  506691.000000  506691.000000  506691.000000  506691.000000   \n",
      "mean        0.057550      -0.045140       0.099533       0.059112   \n",
      "std         1.007066       0.782935       1.092185       1.009313   \n",
      "min        -0.181198      -0.364230      -0.521096      -0.545035   \n",
      "25%        -0.181198      -0.364230      -0.521096      -0.545035   \n",
      "50%        -0.181198      -0.364230      -0.521096      -0.545035   \n",
      "75%        -0.181198      -0.364230       0.986223       1.631139   \n",
      "max        10.500000       4.115924       2.493542       3.807312   \n",
      "\n",
      "             id_37_i        id_38_i   DeviceType_i   DeviceInfo_i  \n",
      "count  506691.000000  506691.000000  506691.000000  506691.000000  \n",
      "mean        0.078374      -0.015629       0.047531       0.226211  \n",
      "std         1.063238       0.904125       1.011508       1.780110  \n",
      "min        -0.522077      -0.524266      -0.528288      -0.162091  \n",
      "25%        -0.522077      -0.524266      -0.528288      -0.162091  \n",
      "50%        -0.522077      -0.524266      -0.528288      -0.162091  \n",
      "75%         1.275445       0.916357       0.852286      -0.162091  \n",
      "max         3.072968       2.356981       2.232861      10.500000  \n",
      "\n",
      "[8 rows x 432 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information about training and testing data\n",
    "* Calcualte the fraction Fraud transactions.\n",
    "* Count the number of variables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud rate: 20663 out of 590540 (3.50%)\n"
     ]
    }
   ],
   "source": [
    "frate = 100. * train[\"isFraud\"].sum() /  len(train)\n",
    "print(\"Fraud rate: %d out of %d (%4.2f%%)\" % \n",
    "      (train[\"isFraud\"].sum(), len(train), frate) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in training:433, testing:432\n",
      "Index(['isFraud', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3',\n",
      "       'card5', 'addr1', 'addr2', 'dist1',\n",
      "       ...\n",
      "       'id_30_i', 'id_31_i', 'id_33_i', 'id_34_i', 'id_35_i', 'id_36_i',\n",
      "       'id_37_i', 'id_38_i', 'DeviceType_i', 'DeviceInfo_i'],\n",
      "      dtype='object', length=433)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of columns in training:%d, testing:%d\" %( len(train.columns), len(test.columns)) )\n",
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5',\n",
      "       'addr1', 'addr2', 'dist1', 'dist2',\n",
      "       ...\n",
      "       'id_30_i', 'id_31_i', 'id_33_i', 'id_34_i', 'id_35_i', 'id_36_i',\n",
      "       'id_37_i', 'id_38_i', 'DeviceType_i', 'DeviceInfo_i'],\n",
      "      dtype='object', length=432)\n"
     ]
    }
   ],
   "source": [
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Assessment\n",
    "What can we learn from the training and testing data before putting them into models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable separation powers\n",
    "Separation power is calculated to understand the difference of two categories of data in given variable. The separation can be used in two situations:\n",
    "* Fraud vs. Non-Fraud: it gives ideas which variables are more powerful in identifying the Fraud Transactions.\n",
    "* Training vs. Testing: it gives ideas which variables are not that consistent in training and testing, thus should be avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation_power(v1 = None, v2 = None, nbins = 10):\n",
    "    \"\"\"\n",
    "    Calculation of separation power for given variables' two distributions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    v1: pd.Series()\n",
    "        the variables' one distribution\n",
    "    v2: pd.Series()\n",
    "        the variables' another distribution\n",
    "    nbins: int\n",
    "        in the calculation, the distributions are binned from its min to max\n",
    "        the number of bins can be changed (default: 10).\n",
    "    Returns:\n",
    "    --------\n",
    "    float: separation power value\n",
    "    \"\"\"\n",
    "    \n",
    "    if v1 is None or v2 is None:\n",
    "        print(\"Cannot calculate seperation power if either variable is None. Return 0.\")\n",
    "        return 0.0\n",
    "    if len(v1) == 0 or len(v2) == 0:\n",
    "        print(\"Length of v1:\", len(v1), \"v2:\", len(v2), \" No separation power!\")\n",
    "        return 0.0\n",
    "    \n",
    "    if nbins < 2:\n",
    "        nbins = 2\n",
    "    # separation defined p25: https://root.cern.ch/download/doc/tmva/TMVAUsersGuide.pdf\n",
    "    vmin = min(v1.min(), v2.min())\n",
    "    vmax = max(v1.max(), v2.max())\n",
    "    binsize = (vmax-vmin)/nbins\n",
    "    ssqr = 0.\n",
    "    # vrng = [ vmin + (idx - 0.5) * binsize for idx in range(nbins+2)]\n",
    "    # count \n",
    "    for idx in range(nbins+1):\n",
    "        imin = vmin + (idx - 0.5) * binsize\n",
    "        b1 = ((v1 >= imin) & (v1 < imin+binsize)).sum()\n",
    "        b1 = float(b1) / len(v1)\n",
    "        b2 = ((v2 >= imin) & (v2 < imin+binsize)).sum()\n",
    "        b2 = float(b2) / len(v2)\n",
    "        if b1 ==0 and b2==0: continue\n",
    "        ssqr = ssqr + (b1 - b2) * (b1 - b2) / (b1 + b2)\n",
    "\n",
    "    return ssqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data separation Fraud vs Non-Fraud\n",
    "The larger this separation is the better the variable can be used for identifying fraud and non-fraud transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 name C10 separation 0.02459643964997889\n",
      "40 name V1 separation 0.11851778379106581\n",
      "60 name V21 separation 0.17521370663654176\n",
      "80 name V41 separation 0.00018675687269592778\n",
      "100 name V61 separation 0.03200563622543222\n",
      "120 name V81 separation 0.18494041279756585\n",
      "140 name V101 separation 0.0023543558683684174\n",
      "160 name V121 separation 0.0013851940443661346\n",
      "180 name V141 separation 0.008229288922090072\n",
      "200 name V161 separation 0.009128074885787285\n",
      "220 name V181 separation 0.05751830531501733\n",
      "240 name V201 separation 0.28437534198663833\n",
      "260 name V221 separation 0.14890481804708203\n",
      "280 name V241 separation 0.17461248129504284\n",
      "300 name V261 separation 0.1873045740141796\n",
      "320 name V281 separation 0.0436589771015734\n",
      "340 name V301 separation 0.011775543654432891\n",
      "360 name V321 separation 0.010090501676662016\n",
      "380 name id_02 separation 0.20342784582085427\n",
      "400 name id_26 separation 0.006384034415852924\n",
      "420 name id_27_i separation 0.005299152905254094\n",
      "Time passed:  0 Minutes 44 Seconds.\n"
     ]
    }
   ],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "cdt0 = datetime.datetime.now()\n",
    "\n",
    "sepdict_train = {}\n",
    "for col, content in train.items():\n",
    "    if col == \"isFraud\": continue\n",
    "    if not is_numeric_dtype(content):\n",
    "        continue\n",
    "    var = train[[col, \"isFraud\"]]\n",
    "    v1 = var[ var[\"isFraud\"] >0 ]\n",
    "    v2 = var[ var[\"isFraud\"] <1 ]\n",
    "    ssqr = separation_power(v1[col], v2[col])\n",
    "    sepdict_train[col] = [ssqr]\n",
    "    if len(sepdict_train) % 20 == 0:\n",
    "        print(len(sepdict_train),\"name\", col, \"separation\", ssqr)\n",
    "separationpd = pd.DataFrame.from_dict(sepdict_train, orient='index', columns=['separation_train_isfraud'])\n",
    "separationpd.sort_values(by=['separation_train_isfraud'], ascending=False, inplace=True)\n",
    "\n",
    "time_dif_1 = datetime.datetime.now() - cdt0\n",
    "print (\"Time passed: %2d Minutes %2d Seconds.\"\\\n",
    "       % ( time_dif_1.total_seconds()//60, time_dif_1.total_seconds()%60) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 best separation variables.\n",
      "             separation_train_isfraud\n",
      "V258                         0.287977\n",
      "V199                         0.287482\n",
      "V201                         0.284375\n",
      "V190                         0.278176\n",
      "V257                         0.277462\n",
      "V200                         0.276125\n",
      "V246                         0.266459\n",
      "V186                         0.262299\n",
      "V189                         0.261477\n",
      "V243                         0.259026\n",
      "V170                         0.254778\n",
      "ProductCD_i                  0.254646\n",
      "V176                         0.253911\n",
      "V188                         0.253672\n",
      "V230                         0.252413\n",
      "V244                         0.251987\n",
      "V242                         0.249132\n",
      "id_35_i                      0.248911\n",
      "id_17                        0.247678\n",
      "V171                         0.242484\n",
      "\n",
      "\n",
      "The bottom 20 best separation variables.\n",
      "       separation_train_isfraud\n",
      "V120               2.447490e-03\n",
      "V173               2.420176e-03\n",
      "V122               2.381656e-03\n",
      "V101               2.354356e-03\n",
      "V334               2.291609e-03\n",
      "V95                2.254818e-03\n",
      "dist1              1.856884e-03\n",
      "V206               1.769693e-03\n",
      "V311               1.639183e-03\n",
      "V269               1.539336e-03\n",
      "V121               1.385194e-03\n",
      "V89                7.407850e-04\n",
      "V28                6.916163e-04\n",
      "V27                6.863484e-04\n",
      "V325               6.529972e-04\n",
      "V68                4.668764e-04\n",
      "V286               2.917764e-04\n",
      "V41                1.867569e-04\n",
      "V305               7.019083e-06\n",
      "V107               1.477661e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"The top 20 best separation variables.\")\n",
    "print(separationpd.head(20))\n",
    "print(\"\\n\")\n",
    "print(\"The bottom 20 best separation variables.\")\n",
    "print(separationpd.tail(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data and testing data separation\n",
    "The smaller this separation is the lower difference it is between training and testing data for the variable. The variables with high separation in training and testing data shouldn't be used for this study, because that would bias the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ranking the variables by train vs test separation power:\n",
      "               separation_train_test\n",
      "TransactionDT               1.954791\n",
      "card4_i                     1.240270\n",
      "M7_i                        0.724378\n",
      "M2_i                        0.541231\n",
      "M4_i                        0.511028\n",
      "M9_i                        0.498900\n",
      "M5_i                        0.407274\n",
      "M8_i                        0.308927\n",
      "M3_i                        0.286788\n",
      "card6_i                     0.255866\n",
      "D15                         0.215991\n",
      "id_13                       0.202369\n",
      "D4                          0.129962\n",
      "ProductCD_i                 0.116852\n",
      "V78                         0.102801\n",
      "V77                         0.102467\n",
      "V87                         0.101863\n",
      "V86                         0.101834\n",
      "V88                         0.101656\n",
      "V66                         0.099123\n",
      "Time passed:  1 Minutes  2 Seconds.\n"
     ]
    }
   ],
   "source": [
    "cdt0 = datetime.datetime.now()\n",
    "\n",
    "sepdict_test = {}\n",
    "for col, content in test.items():\n",
    "    if not is_numeric_dtype(content):\n",
    "        continue\n",
    "    ssqr = separation_power(train[ col ], test[ col ])\n",
    "    sepdict_test[col] = [ssqr]\n",
    "septestpd = pd.DataFrame.from_dict(sepdict_test, orient='index', columns=['separation_train_test'])\n",
    "septestpd.sort_values(by=['separation_train_test'], ascending=False, inplace=True)\n",
    "\n",
    "print(\"\\nRanking the variables by train vs test separation power:\")\n",
    "print(septestpd.head(20))\n",
    "\n",
    "\n",
    "time_dif_1 = datetime.datetime.now() - cdt0\n",
    "print (\"Time passed: %2d Minutes %2d Seconds.\"\\\n",
    "       % ( time_dif_1.total_seconds()//60, time_dif_1.total_seconds()%60) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine two types of separations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining the two separation values!\n",
      "      separation_train_isfraud  separation_train_test\n",
      "V258                  0.287977               0.002569\n",
      "V199                  0.287482               0.003077\n",
      "V201                  0.284375               0.002248\n",
      "V190                  0.278176               0.003110\n",
      "V257                  0.277462               0.002514\n",
      "After sorting\n",
      "               separation_train_isfraud  separation_train_test\n",
      "TransactionDT                  0.020730               1.954791\n",
      "card4_i                        0.005974               1.240270\n",
      "M7_i                           0.079873               0.724378\n",
      "M2_i                           0.130560               0.541231\n",
      "M4_i                           0.208226               0.511028\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining the two separation values!\")\n",
    "sepcombpd = pd.concat([separationpd, septestpd], sort=False, axis=1)\n",
    "print(sepcombpd.head())\n",
    "sepcombpd.sort_values(by=[\"separation_train_test\"], ascending=False, inplace=True)\n",
    "print(\"After sorting\")\n",
    "print(sepcombpd.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection of variables based on the two types of separations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables before selection: 432\n",
      "Selecting variables with low training vs. testing separation to reduce bias.\n",
      "Number of variables after keeping only sep(train, test) < 0.2: 420\n",
      "Number of variables after keeping only sep(train fraud, non-fraud)>0.01: 336\n"
     ]
    }
   ],
   "source": [
    "# only keep variables that has low separation between train and test\n",
    "print(\"Number of variables before selection: %d\"%len(sepcombpd))\n",
    "print(\"Selecting variables with low training vs. testing separation to reduce bias.\")\n",
    "sepsel01 = sepcombpd[ sepcombpd[\"separation_train_test\"] < 0.2 ]\n",
    "print(\"Number of variables after keeping only sep(train, test) < 0.2: %d\"%(len(sepsel01)))\n",
    "separationpd_ok = sepcombpd[ sepcombpd[\"separation_train_isfraud\"] >0.01 ]\n",
    "print(\"Number of variables after keeping only sep(train fraud, non-fraud)>0.01: %d\"%(len(separationpd_ok)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#### %matplotlib notebook\n",
    "#\n",
    "# make a variable plot by comparing isFraud = 0 to 1.\n",
    "#\n",
    "def isFraud_compare(data, categ, varname, xlim = (1.0, -1.0), data_test = None):\n",
    "    var = data[[varname,\"isFraud\"]].dropna()\n",
    "    if xlim[0] < xlim[1]:\n",
    "        vmin, vmax = xlim[0], xlim[1]\n",
    "    else:\n",
    "        vmin, vmax = var[varname].min(), var[varname].max()\n",
    "        if data_test is not None:\n",
    "            vmin = min(vmin, data_test[varname].min())\n",
    "            vmax = max(vmax, data_test[varname].max())\n",
    "    binsize = (vmax-vmin)/9.\n",
    "    vbin = [vmin+(i-0.5)*binsize for i in range(11)]\n",
    "    v1 = var[ var[\"isFraud\"] >0 ]\n",
    "    v2 = var[ var[\"isFraud\"] <1 ]\n",
    "    if len(v1) == 0 or len(v2) == 0:\n",
    "        print(\"Length of var\", varname, \"with fraud:\", len(v1), \"without fraud:\", len(v2), \" Skip!\")\n",
    "        return None\n",
    "    \n",
    "    figx = plt.figure(figsize=(5,5))\n",
    "    plt.hist(v1[varname], weights=[1./len(v1)]*len(v1), bins = vbin, #10, #range=(vmin, vmax), \n",
    "             alpha=0.85, color='grey', \n",
    "             label='Is Fraud: %d (%3.1f%%)'%(len(v1),100*len(v1)*1./(len(v1)+len(v2))) )\n",
    "    plt.hist(v2[varname], weights=[1./len(v2)]*len(v2), rwidth = 0.75, hatch='/', bins = vbin, #10,\n",
    "             alpha=0.75, color='darksalmon', \n",
    "             label='None Fraud: %d (%3.1f%%)'%(len(v2),100*len(v2)*1./(len(v1)+len(v2))) )\n",
    "    if data_test is not None:\n",
    "        plt.hist(data_test[varname], weights=[1./len(data_test)]*len(data_test), rwidth = 0.5, \n",
    "                 hatch='.', bins = vbin, \n",
    "                 alpha=0.65, color='dodgerblue', \n",
    "                 label='Test: %d'%(len(data_test)) )\n",
    "    pltname = \"plot/\"+categ+\"_\"+varname+\"_sb.png\"\n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "    plt.ylim(y1, y2*1.5)\n",
    "    if xlim[0] < xlim[1]:\n",
    "        plt.xlim(xlim[0] -0.5 * binsize, xlim[1] +0.5 * binsize)\n",
    "    plt.xlabel(\"Value of: \"+varname+\" (\"+categ.replace(\"_\", \" \")+\")\")\n",
    "    plt.ylabel(\"Normalized\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"Desity of \"+varname)\n",
    "    #plt.show()\n",
    "    plt.savefig(pltname)\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of variables: 432\n",
      "0 column:  V258\n",
      "50 column:  id_11\n",
      "100 column:  V74\n",
      "150 column:  V255\n",
      "200 column:  V123\n",
      "250 column:  V214\n",
      "300 column:  V284\n",
      "350 column:  V212\n",
      "400 column:  V332\n",
      "Time passed:  1 Minutes 48 Seconds.\n"
     ]
    }
   ],
   "source": [
    "cdt0 = datetime.datetime.now()\n",
    "\n",
    "colname = separationpd.index.values\n",
    "print(\"number of variables:\", len(colname))\n",
    "for idx,col in enumerate(colname):\n",
    "    if idx % 50 == 0:\n",
    "        print(idx, \"column: \", col)\n",
    "    name = \"sep%3d_\" % idx\n",
    "    name = name.replace(\" \", \"0\")\n",
    "    isFraud_compare(train, name, col)\n",
    "    \n",
    "    # use break to draw only one plot, remove for more\n",
    "    #break\n",
    "    \n",
    "time_dif_1 = datetime.datetime.now() - cdt0\n",
    "print (\"Time passed: %2d Minutes %2d Seconds.\"\\\n",
    "       % ( time_dif_1.total_seconds()//60, time_dif_1.total_seconds()%60) )\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#X=data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\n",
    "#y=data['species']  # Labels\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "def train_model( data, ntree = 100, use_prob = False, use_cols = None):\n",
    "    cols = []\n",
    "    for col, content in data.items():\n",
    "        if col == \"isFraud\": continue\n",
    "        if not is_numeric_dtype(content):\n",
    "            continue\n",
    "        if use_cols is not None and col not in use_cols:\n",
    "            continue\n",
    "        cols.append( col )\n",
    "\n",
    "    trnX = data[ cols ] # Features\n",
    "    trnX = trnX.fillna( -1.0 )\n",
    "    trnY = data['isFraud']  # Labels\n",
    "    # Split dataset into training set and test set\n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(trnX, trnY, test_size=0.3)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(trnX, trnY, test_size=0.05)\n",
    "    print(\"Data points: \",len(trnX), len(X_train), len(X_test))\n",
    "    #Create a Gaussian Classifier\n",
    "    clf=RandomForestClassifier(n_estimators = ntree)\n",
    "\n",
    "    #Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    if use_prob:\n",
    "        Y_pred=clf.predict_proba(X_train)\n",
    "        Y_pred=Y_pred[:,1]\n",
    "        print(\"Training precision accuracy:\", metrics.average_precision_score(Y_train, Y_pred))\n",
    "        Y_pred=clf.predict_proba(X_test)\n",
    "        Y_pred=Y_pred[:,1]\n",
    "        print(\"Testing precision accuracy:\", metrics.average_precision_score(Y_test, Y_pred))\n",
    "    else: \n",
    "        Y_pred=clf.predict(X_train)\n",
    "        # Model Accuracy, how often is the classifier correct?\n",
    "        # average_precision_score\n",
    "        print(\"Training accuracy:\", metrics.accuracy_score(Y_train, Y_pred))\n",
    "        Y_pred=clf.predict(X_test)\n",
    "        print(\"Testing accuracy:\", metrics.accuracy_score(Y_test, Y_pred))\n",
    "    return cols, clf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "def train_model_dnn(data, layers=(10,50,100,50,10), itermax=100, alf = 0.0001, tol = 1e-7, \n",
    "                use_prob = False, use_cols = None):\n",
    "    cols = []\n",
    "    for col, content in data.items():\n",
    "        if col == \"isFraud\": continue\n",
    "        if not is_numeric_dtype(content):\n",
    "            continue\n",
    "        if use_cols is not None and col not in use_cols:\n",
    "            continue\n",
    "        cols.append( col )\n",
    "\n",
    "    trnX = data[ cols ] # Features\n",
    "    trnX = trnX.fillna( -1.0 )\n",
    "    trnY = data['isFraud']  # Labels\n",
    "    # Split dataset into training set and test set\n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(trnX, trnY, test_size=0.3)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(trnX, trnY, test_size=0.05)\n",
    "    print(\"Data points: \",len(trnX), len(X_train), len(X_test))\n",
    "    #\n",
    "    clf = MLPClassifier(hidden_layer_sizes=layers, max_iter=itermax, alpha=alf,\n",
    "                        random_state=10101, tol=tol)\n",
    "    #Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    if use_prob:\n",
    "        Y_pred=clf.predict_proba(X_train)\n",
    "        Y_pred=Y_pred[:,1]\n",
    "        print(\"Training precision accuracy:\", metrics.average_precision_score(Y_train, Y_pred))\n",
    "        Y_pred=clf.predict_proba(X_test)\n",
    "        Y_pred=Y_pred[:,1]\n",
    "        print(\"Testing precision accuracy:\", metrics.average_precision_score(Y_test, Y_pred))\n",
    "    else: \n",
    "        Y_pred=clf.predict(X_train)\n",
    "        # Model Accuracy, how often is the classifier correct?\n",
    "        # average_precision_score\n",
    "        print(\"Training accuracy:\", metrics.accuracy_score(Y_train, Y_pred))\n",
    "        Y_pred=clf.predict(X_test)\n",
    "        print(\"Testing accuracy:\", metrics.accuracy_score(Y_test, Y_pred))\n",
    "    return cols, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X=data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\n",
    "#y=data['species']  # Labels\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "def train_model_rfbag( data, ntree = 100, use_prob = True, use_cols = None):\n",
    "    cols = []\n",
    "    for col, content in data.items():\n",
    "        if col == \"isFraud\": continue\n",
    "        if not is_numeric_dtype(content):\n",
    "            continue\n",
    "        if use_cols is not None and col not in use_cols:\n",
    "            continue\n",
    "        cols.append( col )\n",
    "\n",
    "    trnX = data[ cols ] # Features\n",
    "    trnX = trnX.fillna( -1.0 )\n",
    "    trnY = data['isFraud']  # Labels\n",
    "    # Split dataset into training set and test set\n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(trnX, trnY, test_size=0.3)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(trnX, trnY, test_size=0.05)\n",
    "    print(\"Data points: \",len(trnX), len(X_train), len(X_test))\n",
    "    #Create a Gaussian Classifier\n",
    "    #clf= BaggingRegressor(RandomForestClassifier(n_estimators = ntree))\n",
    "    clf= BaggingClassifier(RandomForestClassifier(n_estimators = ntree))\n",
    "    print(\"Model registered.\")\n",
    "    #Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Model trained.\")\n",
    "    if use_prob:\n",
    "        Y_pred_train=clf.predict_proba(X_train)\n",
    "        Y_pred_test=clf.predict_proba(X_test)\n",
    "    else: \n",
    "        Y_pred_train=clf.predict(X_train)\n",
    "        Y_pred_test=clf.predict(X_test)\n",
    "    return (cols, clf, Y_train, Y_pred_train, Y_test, Y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 336\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# use only the leading 100 variables in separation power\n",
    "#\n",
    "#cols_select_outer = sep_train_outer.index[0:500].get_values()\n",
    "cols_select = separationpd_ok.index.get_values()\n",
    "print(type(cols_select), len(cols_select))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points:  590540 561013 29527\n",
      "Model registered.\n",
      "Model trained.\n",
      "Time passed: 237 Minutes 47 Seconds.\n"
     ]
    }
   ],
   "source": [
    "cdt0 = datetime.datetime.now()\n",
    "\n",
    "res = train_model_rfbag(train, ntree = 500, use_prob = True, use_cols = cols_select)\n",
    "cols_prob, model_prob, Ytrain, Ytrain_prd, Ytest, Ytest_prd = res\n",
    "\n",
    "time_dif_1 = datetime.datetime.now() - cdt0\n",
    "print (\"Time passed: %2d Minutes %2d Seconds.\"\\\n",
    "       % ( time_dif_1.total_seconds()//60, time_dif_1.total_seconds()%60) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision accuracy: 0.9962975647686902\n"
     ]
    }
   ],
   "source": [
    "# print the acuracy matrix\n",
    "Ytrain_prd=Ytrain_prd[:,1]\n",
    "print(\"Training precision accuracy:\", metrics.average_precision_score(Ytrain, Ytrain_prd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ytest_prd=Ytest_prd[:,1]\n",
    "print(\"Testing precision accuracy:\", metrics.average_precision_score(Ytest, Ytest_prd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here comes the testing data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "__tmp = count_missing(test_trans)\n",
    "print(\"\\n============\\n\")\n",
    "__tmp = count_missing(test_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#### %matplotlib notebook\n",
    "#\n",
    "# make a variable plot by comparing training to testing data\n",
    "#\n",
    "def train_test_compare(data_train, data_test, categ, varname, xlim = (1.0, -1.0)):\n",
    "    if xlim[0] < xlim[1]:\n",
    "        vmin, vmax = xlim[0], xlim[1]\n",
    "    else:\n",
    "        vmin, vmax = data_train[varname].min(), data_train[varname].max()\n",
    "        vmin = min(vmin, data_test[varname].min())\n",
    "        vmax = max(vmax, data_test[varname].max())\n",
    "    binsize = (vmax-vmin)/9.\n",
    "    vbin = [vmin+(i-0.5)*binsize for i in range(11)]\n",
    "    \n",
    "    figx = plt.figure(figsize=(5,5))\n",
    "    plt.hist(data_train[varname], weights=[1./len(data_train)]*len(data_train), bins = vbin, #10, #range=(vmin, vmax), \n",
    "             alpha=0.85, color='grey', \n",
    "             label='Train Data: %d'%(len(data_train)) )\n",
    "    plt.hist(data_test[varname], weights=[1./len(data_test)]*len(data_test), rwidth = 0.5, \n",
    "             hatch='.', bins = vbin, \n",
    "             alpha=0.65, color='dodgerblue', \n",
    "             label='Test Data: %d'%(len(data_test)) )\n",
    "    pltname = \"plot/\"+categ+\"_\"+varname+\"_sb.png\"\n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "    plt.ylim(y1, y2*1.5)\n",
    "    if xlim[0] < xlim[1]:\n",
    "        plt.xlim(xlim[0] -0.5 * binsize, xlim[1] +0.5 * binsize)\n",
    "    plt.xlabel(\"Value of: \"+varname+\" (\"+categ.replace(\"_\", \" \")+\")\")\n",
    "    plt.ylabel(\"Normalized\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"Desity of \"+varname)\n",
    "    #plt.show()\n",
    "    plt.savefig(pltname)\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed:  1 Minutes 36 Seconds.\n"
     ]
    }
   ],
   "source": [
    "cdt0 = datetime.datetime.now()\n",
    "testcolname = septestpd.index.values\n",
    "#figure with test\n",
    "for idx,col in enumerate(testcolname):\n",
    "    name = \"test_sep%3d\" % idx\n",
    "    name = name.replace(\" \", \"0\")\n",
    "    train_test_compare(train, test, name, col)\n",
    "    \n",
    "time_dif_1 = datetime.datetime.now() - cdt0\n",
    "print (\"Time passed: %2d Minutes %2d Seconds.\"\\\n",
    "       % ( time_dif_1.total_seconds()//60, time_dif_1.total_seconds()%60) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5',\n",
      "       'addr1', 'addr2', 'dist1', 'dist2',\n",
      "       ...\n",
      "       'id_30_i', 'id_31_i', 'id_33_i', 'id_34_i', 'id_35_i', 'id_36_i',\n",
      "       'id_37_i', 'id_38_i', 'DeviceType_i', 'DeviceInfo_i'],\n",
      "      dtype='object', length=432)\n"
     ]
    }
   ],
   "source": [
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model( data, cols_model, clf, use_prob = True):\n",
    "    cols = []\n",
    "    for col, content in data.items():\n",
    "        if col == \"isFraud\": continue\n",
    "        if not is_numeric_dtype(content):\n",
    "            continue\n",
    "        # if one variable is not in training model, skip\n",
    "        if col not in cols_model:\n",
    "            #print(col,\"NOT found in training model. Skip.\")\n",
    "            continue\n",
    "        cols.append( col )\n",
    "\n",
    "    tstX = data[ cols ] # Features\n",
    "    tstX = tstX.fillna( -1.0 )\n",
    "    print(\"Data points: \",len(tstX))\n",
    "    if use_prob:\n",
    "        Y_pred=clf.predict_proba(tstX)\n",
    "        Y_pred=Y_pred[:,1]\n",
    "    else:\n",
    "        Y_pred=clf.predict(tstX)\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points:  506691\n"
     ]
    }
   ],
   "source": [
    "Ytst_prob = test_model(test, cols_select, model_prob, use_prob = True)\n",
    "result_prob = pd.DataFrame(Ytst_prob, index = test.index, columns=[\"isFraud\"])\n",
    "result_prob.sort_index(inplace=True)\n",
    "result_prob.to_csv(\"out_prob.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
